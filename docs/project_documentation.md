# Project Documentation: Emotion Recognition Model

## Aim of the Project

This document outlines the workflow, development process, and results of a project focused on building and optimizing an emotion recognition model. The primary aim was to identify an effective deep learning model for classifying facial expressions, fine-tune it for improved accuracy on a comprehensive dataset, and subsequently explore quantization techniques to reduce model size and assess its impact on performance and latency, particularly for potential deployment on resource-constrained devices. The project involved multiple phases, including initial model selection, fine-tuning with a more diverse dataset, and a detailed ablation study comparing the original and quantized model versions using both standard and custom-collected unseen data.



## Baseline: Phase 1 - Initial Model Selection

The initial phase of the project, designated as Phase 1, was focused on establishing a baseline by identifying the most suitable deep learning model architecture for the task of emotion recognition. This phase was conducted on the CSC Puhti supercomputer to leverage its high computational power and parallel processing capabilities, which were essential for efficiently training and evaluating multiple candidate models. The workflow was orchestrated using Slurm batch scripts.

### Phase 1 Workflow Orchestration (`train.emotion_phase1.slurm`)

The `train.emotion_phase1.slurm` script served as the master script for managing the execution of Phase 1. Its primary role was to automate the training and evaluation of a predefined set of five different model architectures: EfficientNet-B0, ResNet50, MobileNetV3-Small, MobileViT-XXS, and DenseNet121. The script began by setting up the necessary environment on the Puhti cluster, which included defining project-specific paths, locating the FER-2013 dataset (which was the designated dataset for this phase), and configuring directories for storing logs, trained models, performance metrics, and checkpoints. It managed software dependencies by loading the required PyTorch module and ensuring the installation of auxiliary Python packages like `omegaconf` for configuration management and `tqdm` for progress tracking. A key feature of this Slurm script was its use of job arrays. This allowed for the parallel training of each candidate model, significantly speeding up the model selection process. Each model training instance was treated as an independent task within the array, with dedicated output and error logging. To optimize I/O operations during training, the script copied the core Python training logic (`train_phase1.py`) to the compute node's local fast storage ($LOCAL_SCRATCH) before execution. The `srun` command was then used to launch the `train_phase1.py` script, passing it various parameters such as the specific model to train, a configuration file (`phase1.yaml`), an instruction to utilize pretrained weights, the data directory path, batch size, and locations for output files. Finally, the script ensured that all results and artifacts generated in the temporary local scratch space were diligently copied back to the persistent project storage upon job completion. This systematic approach facilitated a fair and efficient comparison of the candidate models on the FER-2013 dataset, laying the groundwork for selecting the best performer for the subsequent fine-tuning phase.




### Phase 1 Core Training Logic (`train_phase1.py`)

The Python script `train_phase1.py` was the engine driving the model training and evaluation during Phase 1. Its central aim was to systematically train several deep learning architectures on the FER-2013 dataset and evaluate their performance to identify the most promising candidate for emotion classification. The script was designed for flexibility, parsing command-line arguments and loading detailed configurations from an associated YAML file (`phase1.yaml`). This allowed for precise control over crucial aspects of the training process, such as the choice of model, data directory paths, batch size, the number of training epochs, and learning rate parameters.

To handle data loading and preprocessing, the script defined a custom PyTorch `Dataset` class named `EmotionDataset`. This class was specifically tailored to work with the FER-2013 dataset, efficiently managing the 48x48 pixel grayscale images and their corresponding emotion labels. A significant part of the preprocessing pipeline involved data augmentation for the training set. Techniques such as random resized cropping, random horizontal flipping, random rotations, color jitter, and random erasing were implemented to enhance the diversity of the training data and thereby improve the model's ability to generalize to unseen images. For the validation and test datasets, more standard preprocessing steps were applied, including resizing, center cropping, and normalization, to ensure consistent evaluation.

The script was equipped to instantiate and train a variety of well-known Convolutional Neural Network (CNN) architectures. These included EfficientNet-B0, MobileNetV3-Small, ResNet50, DenseNet121, and MobileViT-XXS. For each of these models, the script leveraged pretrained weights (typically from ImageNet) to benefit from learned features, and then adapted their final classification layers to suit the specific requirements of the emotion recognition task, which in Phase 1 involved classifying images into six emotion categories: anger, fear, happy, sad, surprise, and neutral.

The core training loop was encapsulated within a `train_model` function. This function implemented a standard PyTorch training regimen, iterating through the specified number of epochs. Each epoch consisted of a training phase, where the model weights were updated, and a validation phase, where the model's performance was assessed on a separate data split without updating weights. The script employed CrossEntropyLoss as the loss function, suitable for multi-class classification, and utilized the AdamW optimizer, known for its effectiveness in training deep neural networks, in conjunction with a CosineAnnealingLR learning rate scheduler to dynamically adjust the learning rate during training. Throughout this process, the script meticulously tracked both training and validation loss and accuracy. It also implemented a mechanism to save the model weights that achieved the highest validation accuracy. For enhanced monitoring and visualization of the training progress, TensorBoard logging was integrated.

Upon completion of the training epochs, an `evaluate_model` function was invoked to assess the final performance of the trained model on a dedicated test set. This function calculated and reported a comprehensive suite of evaluation metrics. These included overall accuracy, as well as weighted precision, recall, and F1-score, which provide a balanced view of the model's performance, especially in the presence of class imbalances. Furthermore, the script generated a detailed confusion matrix and, importantly, provided per-class metrics (precision, recall, F1-score for each of the six emotions). This granular level of detail was crucial for understanding the model's strengths and weaknesses in recognizing specific emotions.

To ensure the reproducibility of the experiments, the script took care to set random seeds for PyTorch and NumPy. It also managed the creation of a structured hierarchy of output directories. These directories were used to store various artifacts from the training process, including logs, the saved model files (`.pt`), evaluation metrics, and training checkpoints. All outputs were organized by the specific model architecture being trained and were timestamped for easy identification and comparison. The final trained model, along with its associated configuration parameters and training history, was saved to a file, providing a complete record of each training run. In essence, `train_phase1.py` provided the robust and automated framework necessary for the systematic training and rigorous evaluation of multiple candidate models, thereby generating the empirical evidence required to select the best-performing architecture for advancement to Phase 2 of the project.




## Development: Phase 2 - Model Finetuning and Optimization

Following the selection of EfficientNet-B0 as the best-performing model architecture from Phase 1, the project transitioned into Phase 2. This phase was dedicated to further refining the chosen model by fine-tuning it on a different and more comprehensive dataset, the Real-world Affective Faces Database (RAF-DB), specifically its single-label subset. This dataset introduced an additional emotion category, 'disgust,' expanding the classification task to seven emotions. Similar to Phase 1, Phase 2 operations were conducted on the CSC Puhti supercomputer, utilizing its GPU resources for efficient training. The workflow was again managed by a Slurm script, and the fine-tuned model was subsequently prepared for quantization and detailed performance analysis.

### Phase 2 Workflow Orchestration (`train.emotion_phase2.slurm`)

The `train.emotion_phase2.slurm` script was responsible for managing the execution of the fine-tuning process in Phase 2. Its primary objective was to take the EfficientNet-B0 model, identified as optimal in Phase 1, and further train it using the RAF-DB dataset with a specific set of hyperparameters to enhance its emotion recognition capabilities. The script began by configuring the Puhti environment, requesting necessary computational resources such as a V100 GPU, an appropriate number of CPUs, and sufficient memory, along with setting a job time limit. It ensured the correct software environment by loading a specific version of PyTorch (2.0), which was required for the Phase 2 training script. 

A key aspect of this Slurm script was the precise definition of parameters for the fine-tuning process. These included the paths to the processed RAF-DB dataset and its corresponding label file, the base directory for saving the resulting fine-tuned models, and a suite of hyperparameters tailored for this phase. The user-specified hyperparameters for the final successful run were: `--model efficientnet_b0 --batch_size 16 --epochs 30 --lr 0.0005 --weight_decay 1e-3`. The script also handled other training-related parameters such as the number of data loader workers, mixup alpha for data augmentation, early stopping patience, and a label smoothing factor. It explicitly set the model to `efficientnet_b0` and included an option to enable class balancing to address potential imbalances in the dataset. 

For effective management of experimental results, the script created a unique, timestamped output directory for each training run. This practice helped in organizing the models and logs from different fine-tuning attempts. Standard output and error streams were directed to files named with the job ID for straightforward tracking and debugging. The core execution step involved launching the Phase 2 Python training script (which corresponds to the provided `train_phase2local.py`) using `srun`. All the predefined parameters, paths, and hyperparameters were passed as command-line arguments to this Python script. 

Upon completion of the training script, the Slurm script incorporated robust result handling. It checked the exit code of the training process to determine success or failure. If the training was successful, it verified the creation and location of the saved model file (typically `model.pt`). A crucial feature was writing the full path of this newly fine-tuned model to a text file named `latest_model_path.txt`. This mechanism was designed to allow subsequent scripts, particularly the model optimization and quantization script, to easily and automatically locate the most recent and relevant fine-tuned model. If the training process failed, or if the expected model file was not found, the script logged this information and listed the contents of the output directory to aid in troubleshooting. In the overall project pipeline, `train.emotion_phase2.slurm` automated the critical fine-tuning stage, ensuring that the selected model was meticulously refined on a new dataset with optimized settings, thereby producing the model that would then proceed to the quantization and ablation study phase.




### Phase 2 Core Training Logic (`train_phase2local.py`)

The Python script `train_phase2local.py` (referred to as `src/train_phase2.py` within the Slurm execution environment) was the cornerstone of the model fine-tuning activities in Phase 2. Its central purpose was to take the EfficientNet-B0 model, which had been identified as the top performer in Phase 1, and further enhance its capabilities by fine-tuning it on the RAF-DB dataset. This dataset introduced a seventh emotion category, 'disgust,' making the classification task more nuanced. The script was engineered to be more robust than its Phase 1 counterpart and incorporated a range of advanced training techniques to maximize performance.

At its core, the script provided flexible configuration management. It was capable of loading parameters from a YAML configuration file (if the OmegaConf library was available) and also allowed these settings to be overridden via command-line arguments. This provided granular control over critical aspects such as data paths, the specific model architecture to be used (though consistently EfficientNet-B0 in this phase as per the project flow), batch size, the number of training epochs, learning rate, weight decay, and various flags for enabling or disabling specific training methodologies.

A sophisticated custom PyTorch `Dataset` class, also named `EmotionDataset` but adapted for RAF-DB, was a key component. This class was specifically designed to handle the intricacies of the RAF-DB dataset, including its unique label mapping (where numeric labels 1-7 correspond to specific emotion strings). It incorporated robust mechanisms for validating image paths and ensuring that only valid images and their corresponding labels were utilized during training. This resilience was important for handling potential inconsistencies in dataset structure or label file formats.

The script explicitly defined seven emotion classes for this phase: 'anger', 'fear', 'happy', 'sad', 'surprise', 'neutral', and 'disgust'. While the primary model for fine-tuning was EfficientNet-B0, the script also contained code for an `EmotionAttentionNetwork`. This custom architecture, which could leverage EfficientNet-B0 or EfficientNet-B2 as a backbone, was designed to incorporate attention mechanisms such as CBAM (Convolutional Block Attention Module) and SEBlock (Squeeze-and-Excitation Block). These attention mechanisms were intended to help the model focus on more salient facial regions, potentially improving feature extraction and, consequently, emotion recognition accuracy. The script also retained support for standard torchvision models, always adapting their final classification layers to the seven-emotion task.

Data augmentation and preprocessing were handled using the `torchvision.transforms.v2` library, which offers modern and potentially more efficient transformation capabilities. The augmentation pipeline for the training data included techniques such as resizing, random resized cropping, random horizontal flipping, random affine transformations (incorporating rotations and translations), and color jitter. Normalization was applied consistently. For the validation set, a less aggressive set of transformations, typically involving resizing, center cropping, and normalization, was used to ensure a fair evaluation of the model's learned capabilities.

The training loop, encapsulated in the `train_model` function, was significantly enhanced with several advanced training strategies. Mixup data augmentation was implemented, a technique that trains the model on linear interpolations of pairs of examples and their labels, often leading to improved generalization. Label smoothing was applied to the CrossEntropyLoss function, a regularization technique that helps prevent the model from becoming overconfident in its predictions. To ensure training stability, gradient clipping was used to prevent the gradients from becoming excessively large. If a CUDA-enabled GPU was available, the script leveraged Automatic Mixed Precision (AMP) training via `torch.cuda.amp.GradScaler`, which can accelerate training and reduce memory consumption by using a combination of full (FP32) and half-precision (FP16) floating-point numbers. An early stopping mechanism was also in place; this monitored the validation performance (typically F1-score, which is robust for potentially imbalanced datasets, or accuracy) and would halt the training process if no improvement was observed for a predefined number of epochs (patience), thereby saving computational resources and preventing overfitting. The best model weights encountered during training were saved. A learning rate scheduler, such as ReduceLROnPlateau or CosineAnnealingLR (as indicated by the Slurm script parameters for the final run), was employed to dynamically adjust the learning rate, and the AdamW optimizer was used for its often superior generalization performance.

The `evaluate_model` function was responsible for assessing the fine-tuned model's performance on the test or validation set. It calculated and reported a comprehensive set of metrics, including overall accuracy, weighted precision, recall, and F1-score, along with a detailed confusion matrix. Critically, it also provided per-class precision, recall, and F1-scores for each of the seven emotions, offering deep insights into the model's performance characteristics across different emotional states. Although the primary execution path involved a standard train/validation split as dictated by the Slurm script, the Python script also contained the underlying functionality for performing k-fold cross-validation, which could be used for more robust model evaluation if required.

To ensure the consistency and reproducibility of the fine-tuning process, the script set random seeds for PyTorch, NumPy, and Python's built-in random module. It also supported TensorBoard logging for real-time visualization of training metrics. Upon completion, the final fine-tuned model, along with the optimizer state, the history of training metrics, and relevant configuration details, was saved to a timestamped output directory. This meticulous saving process ensured that all aspects of the fine-tuning run were preserved for later analysis and for use in the subsequent quantization phase. In summary, `train_phase2local.py` provided the advanced framework necessary to rigorously fine-tune the selected EfficientNet-B0 model on the more challenging RAF-DB dataset, aiming to produce a highly accurate and robust emotion recognition model ready for the final optimization and evaluation stages.




### Model Optimization and Ablation Study (`optimize_model_original.py`)

The `optimize_model_original.py` script represented the culminating stage of the model development pipeline prior to any potential deployment. Its core focus was on optimizing the fine-tuned emotion recognition model from Phase 2 and conducting a detailed performance comparison. The primary objective was to take the highly accurate EfficientNet-B0 model, apply quantization to reduce its size and potentially improve inference speed, and then carry out an ablation study. This study was designed to meticulously compare the performance characteristics—specifically accuracy, model size, and inference latency—of the original full-precision (FP32) model against its quantized (INT8) counterpart.

A series of key functionalities were embedded within this script. It began by loading the trained PyTorch model checkpoint that was saved at the conclusion of Phase 2. During this loading process, it dynamically ascertained the model architecture (e.g., `efficientnet_b0`) and the number of emotion classes directly from the information stored in the checkpoint file.

The script then proceeded to implement dynamic INT8 quantization for the loaded model. This quantization technique aims to significantly reduce the model's storage footprint and can also lead to faster inference times, particularly on hardware that supports INT8 operations, all while striving to minimize any loss in predictive accuracy. The quantization process specifically targeted the linear and convolutional layers of the neural network and was performed on the CPU, as is common for many deployment scenarios.

To facilitate a thorough comparison, the script included functions to measure several critical performance metrics. It calculated the file size, in megabytes (MB), of both the original FP32 model and the newly created quantized INT8 model. Furthermore, it incorporated a robust method for measuring inference latency. This involved running inference multiple times (defaulting to 30 runs, as specified for the ablation study) on a given input tensor and recording the average, minimum, and maximum inference times in milliseconds. This latency measurement was performed for both model versions; typically, the original model would be benchmarked on a GPU if available (as was the case in the CSC Puhti environment), while the quantized model, often intended for CPU-bound deployment, was benchmarked on the CPU.

An evaluation function was also defined to assess the predictive performance of both the original and quantized models on a given dataset. This function calculated standard classification metrics, including accuracy, and weighted precision, recall, and F1-score. These metrics were crucial for understanding any impact of quantization on the model's ability to correctly classify emotions and were used extensively within the ablation study.

For the ablation study itself, the script was designed to handle multiple datasets. It loaded and utilized both the RAF-DB dataset (which the model was fine-tuned on in Phase 2) and a custom dataset. This custom dataset, with images manually collected from Pexels, was specifically included to serve as unseen data, providing a more stringent test of the models' generalization capabilities. The script created PyTorch DataLoaders for both these datasets to efficiently feed images to the models during the study.

The `run_ablation_study` function was the centerpiece of this script. It systematically compared the original and quantized models across a predefined number of experiments (30, as per the user's instruction and the provided `ablation_study.md` log). In each experiment, a batch of images was processed, with the script alternating between drawing images from the RAF-DB dataset and the custom Pexels dataset. For every image within each batch, the script meticulously recorded the predictions made by both the original and the quantized models, whether these predictions were correct when compared against the true labels, and the inference latency for both models on that specific batch. All these detailed results, including the overall model sizes, average latencies across experiments, and the per-experiment and per-image prediction outcomes, were compiled and saved into a structured JSON file named `ablation_study.json`. The script also printed summary statistics to the console, offering an immediate comparison of the overall performance of the two model versions.

Although not a primary focus of the user-requested documentation points, the script also contained functionality to export a model to the TorchScript Lite format. This format is optimized for deployment on mobile devices, such as those running Android. The export process involved tracing the model with a sample input and saving the resulting traced model. A verification step for the exported TorchScript model was also included to ensure its integrity. In the context of the project's lifecycle, the `optimize_model_original.py` script provided the critical empirical data necessary for making informed decisions about the deployability of the emotion recognition model. By rigorously comparing the full-precision model with its quantized version, it offered valuable insights into the inherent trade-offs between model size, inference speed, and predictive accuracy, directly informing the conclusions drawn from the `ablation_study.md` and guiding future deployment strategies.




## Results: Fine-tuned Model Performance

Following the successful completion of Phase 2, which involved fine-tuning the EfficientNet-B0 model with the RAF-DB dataset and optimized hyperparameters (`--model efficientnet_b0 --batch_size 16 --epochs 30 --lr 0.0005 --weight_decay 1e-3`), the project achieved significant performance in emotion recognition across seven distinct emotion categories. The evaluation of this fine-tuned model yielded detailed metrics for each emotion, reflecting a robust understanding of nuanced facial expressions.

For the emotion 'anger,' the model demonstrated a precision of 0.7751, a recall of 0.7360, and an F1-score of 0.7550. This indicates a strong ability to correctly identify instances of anger while maintaining a good balance between precision and recall.

In recognizing 'fear,' the model achieved a high precision of 0.7872. However, the recall was 0.5068, leading to an F1-score of 0.6167. While the model was accurate when it predicted fear, it missed a notable portion of actual fear instances.

The model showed exceptional performance for the 'happy' emotion, with a precision of 0.9279, recall of 0.9381, and an F1-score of 0.9329. These high scores suggest that the model is very effective and reliable in identifying happiness.

For 'sad' expressions, the model obtained a precision of 0.7833, a recall of 0.8408, and an F1-score of 0.8110. The high recall indicates that the model was particularly good at capturing most instances of sadness.

The 'surprise' emotion was also well-recognized, with a precision of 0.8394, recall of 0.8629, and an F1-score of 0.8510, demonstrating a strong and balanced performance for this category.

For 'neutral' expressions, the model achieved a precision of 0.8115, recall of 0.8264, and an F1-score of 0.8189, indicating consistent and reliable identification.

Lastly, for the 'disgust' emotion, the model registered a precision of 0.6879, recall of 0.5511, and an F1-score of 0.6120. Similar to 'fear,' while the precision is reasonable, the recall suggests that the model found 'disgust' more challenging to detect comprehensively, or that instances of disgust were less distinctly represented in the dataset compared to other emotions.

Overall, these results highlight the success of the fine-tuning phase in developing a capable emotion recognition model, with particularly strong performance for happiness, surprise, sadness, and neutral expressions, and identifies areas such as fear and disgust where further targeted improvements could be beneficial.




## Model Quantization and Ablation Study Findings

Following the fine-tuning of the EfficientNet-B0 model, a critical phase of the project involved model quantization and a subsequent ablation study to evaluate its impact. The primary goals of quantization were to achieve a smaller model size suitable for potential mobile deployment, while endeavoring to maintain high accuracy and not significantly compromise latency. The `optimize_model_original.py` script was responsible for performing dynamic INT8 quantization on the fine-tuned model and then conducting this ablation study.

The ablation study was comprehensive, encompassing 30 distinct experiments. These experiments were designed to rigorously compare the performance of the original full-precision (FP32) model with its quantized (INT8) counterpart. The evaluation was performed using batches of images drawn alternately from two datasets: the RAF-DB dataset (on which the model was fine-tuned in Phase 2) and a custom dataset. This custom dataset, manually collected from Pexels, served as unseen data to assess the models' generalization capabilities.

Key findings from the `ablation_study.md` log, which detailed the per-experiment batch accuracies and latencies, are as follows:

1.  **Accuracy Preservation**: Across the 30 experiments, the dynamic INT8 quantization demonstrated remarkable success in preserving the predictive accuracy of the model. For numerous batches, particularly from the RAF-DB dataset, the batch accuracy of the quantized model was identical to that of the original model (e.g., often 100% or 80% for both). Similar trends were observed on the custom (unseen) dataset, where both models typically achieved identical, albeit sometimes lower, batch accuracies (e.g., 60% or 80%). This indicates that the quantization process did not lead to a significant degradation in the model's ability to correctly classify emotions, even on previously unseen data.

2.  **Latency Considerations**: The ablation study reported latency measurements for both models. The original FP32 model was typically evaluated on a GPU (if available), while the quantized INT8 model was evaluated on the CPU. Under this specific comparison setup (GPU FP32 vs. CPU INT8), the quantized model exhibited noticeably higher average inference latencies per batch (e.g., original model latencies were often in the range of 5-10 ms, while quantized model latencies were frequently in the 40-50 ms range, with some outliers). This is an important observation. While one of the goals was not to compromise latency, this comparison highlights the performance difference between GPU and CPU execution. If the target deployment is CPU-bound, the relevant comparison would be FP32 CPU vs. INT8 CPU, which might show a latency benefit for the INT8 model on CPU. The provided study focuses on the GPU vs. CPU scenario.

3.  **Performance on Custom Unseen Dataset**: Both the original and quantized models were tested on the custom dataset. As expected with unseen data, the batch accuracies were sometimes lower than those observed on the RAF-DB dataset. However, crucially, the quantized model generally mirrored the performance of the original model on this custom data, suggesting that quantization did not disproportionately harm its generalization to new, diverse images.

4.  **Model Size Reduction**: Although not detailed within the `ablation_study.md` log itself (which focused on runtime predictions and latencies), a primary motivation for quantization, as handled by the `optimize_model_original.py` script, is the reduction in model file size. This is a significant advantage for deployment on resource-constrained environments like mobile devices, and the quantization process is expected to have achieved this.

In summary, the ablation study, conducted over 30 experiments with both known (RAF-DB) and unseen (custom Pexels) data, indicated that dynamic INT8 quantization was highly effective in maintaining the accuracy of the fine-tuned EfficientNet-B0 model. The main trade-off observed in the provided logs, when comparing GPU-run original model versus CPU-run quantized model, was an increase in latency for the quantized version. The study underscores the viability of the quantized model in terms of accuracy retention, which, coupled with the expected benefit of reduced size, makes it a candidate for further deployment considerations, especially if CPU-based inference is the target or if the size reduction is paramount.




## References

1.  **FER-2013 Dataset**: Goodfellow, I. J., Erhan, D., Carrier, P. L., Courville, A., Mirza, M., Hamner, B., Cukierski, W., Tang, Y., Thaler, D., Lee, D.-H., Zhou, Y., Ramaiah, C., Feng, F., Li, R., Wang, X., Athanasakis, D., Shawe-Taylor, J., Milakov, M., Park, J., Ionescu, R., Popescu, M., Grozea, C., Bergstra, J., Xie, J., Romaszko, L., Xu, B., Chuang, Z., & Bengio, Y. (2013). *Challenges in Representation Learning: A report on three machine learning contests*. Paper presented at the ICML 2013 Workshop on Challenges in Representation Learning. Retrieved from [https://paperswithcode.com/paper/challenges-in-representation-learning-a](https://paperswithcode.com/paper/challenges-in-representation-learning-a)

2.  **RAF-DB (Real-world Affective Faces Database)**: Li, S., Deng, W., & Du, J. (2017). Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)* (pp. 2252-2260). DOI: 10.1109/CVPR.2017.243. The dataset is available from [http://www.whdeng.cn/RAF/model1.html](http://www.whdeng.cn/RAF/model1.html).

3.  **Custom Dataset Source (Pexels)**: Pexels. (n.d.). *Free Stock Photos, Royalty Free Stock Images & Copyright Free Pictures*. Retrieved May 16, 2025, from [https://www.pexels.com](https://www.pexels.com). (Images used for the custom dataset were manually collected from Pexels, which provides royalty-free stock photos under the Pexels license).

4.  **Computational Resources**: The computational resources for Phase 1 and Phase 2 of this project, including model training on the Puhti supercomputer, were provided by CSC – IT Center for Science, Finland (Project ID: project_2014146).

5.  **Primary Software Libraries and Tools**:
    *   **PyTorch**: Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., ... & Chintala, S. (2019). PyTorch: An Imperative Style, High-Performance Deep Learning Library. In *Advances in Neural Information Processing Systems 32* (pp. 8026-8037). Retrieved from [https://pytorch.org/](https://pytorch.org/)
    *   **Slurm Workload Manager**: Yoo, A. B., Jette, M. A., & Grondona, M. (2003). SLURM: Simple Linux Utility for Resource Management. In *Job Scheduling Strategies for Parallel Processing* (pp. 44-60). Springer Berlin Heidelberg. Retrieved from [https://slurm.schedmd.com/](https://slurm.schedmd.com/)
    *   **EfficientNet (as a model architecture)**: Tan, M., & Le, Q. V. (2019). EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. In *Proceedings of the 36th International Conference on Machine Learning (ICML)* (pp. 6105-6114). PMLR.

